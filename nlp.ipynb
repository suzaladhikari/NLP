{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regular Expressions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regular expressions helps us to find the pattern in the given string which helps to tackle the classification problems\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<re.Match object; span=(0, 5), match='sujal'>\n"
     ]
    }
   ],
   "source": [
    "mystring = 'sujal'\n",
    "print(re.match(mystring, 'sujal adhikari is the man')) \n",
    "## Here the match is sujal or sujal is the common between the two strings \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<re.Match object; span=(0, 5), match='Peter'>\n"
     ]
    }
   ],
   "source": [
    "used_regex = '\\w+'\n",
    "print(re.match(used_regex, \"Peter Fernandez\"))\n",
    "## Here only peter is printed becuase the space is not the string or the character"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Common regex patterns \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "## There are tons of patterns in the regex but the most common are \n",
    "#print(re.match('\\w+', 'Sujal Adhikari ')) ## Gives all the strings before the character that is not space\n",
    "print(re.match('\\d+', 'DOB 2005'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization is the process of breaking down the strings into chunks in order to make it easy for the strings "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hey', 'there', 'fellas', 'Sujal', 'Adhikari', 'here']\n"
     ]
    }
   ],
   "source": [
    "## Using the normal regex library \n",
    "my_string = \"Hey there fellas ! Sujal Adhikari here!\"\n",
    "print(re.findall(r'\\w+', my_string)) ## So basically, it turns the whole sentence into the tokens \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regex or regular expressions only work on strings "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NLTK -> Natural Language Toolkit \n",
    "### It is one of the handy library which is used to do tokenization, stemming, lemmatization, parsing and many more "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<re.Match object; span=(19, 29), match='8627038353'>\n"
     ]
    }
   ],
   "source": [
    "number = \"My phone number is 8627038353\"\n",
    "expression = r'\\d{10}'\n",
    "print(re.search(expression,number)) ## The output is the number "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'admires', 'He', 'the', 'who', 'Peter', '.', 'everyone', 'is', 'man'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /Users/sujaladhikari/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize,regexp_tokenize\n",
    "scene_one = 'Peter is the man who everyone admires. He is the man'\n",
    "nltk.download('punkt_tab') \n",
    "# Split scene_one into sentences: sentences\n",
    "sentences = sent_tokenize(scene_one)\n",
    "\n",
    "# Use word_tokenize to tokenize the fourth sentence: tokenized_sent\n",
    "tokenized_sent = word_tokenize(sentences[1])\n",
    "\n",
    "# Make a set of unique tokens in the entire scene: unique_tokens\n",
    "unique_tokens = set(word_tokenize(scene_one))\n",
    "\n",
    "# Print the unique tokens result\n",
    "print(unique_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regex groupping using the letter '|'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['He', 'has', '11', 'cats']\n"
     ]
    }
   ],
   "source": [
    "match_digits_and_words = ('(\\d+|\\w+)')\n",
    "print(re.findall(match_digits_and_words, 'He has 11 cats'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some useful tokenization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['NAME', 'SUJAL', 'ADHIKARI', 'Age', 'Twenty']\n"
     ]
    }
   ],
   "source": [
    "string_to_be_tokenized = 'NAME: SUJAL ADHIKARI, Age: 20, Twenty.'\n",
    "print(re.findall('[A-Za-z]+',string_to_be_tokenized)) ## These are the ones with the upper and lower case in it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['NAME:', 'SUJAL', 'ADHIKARI', 'Age:', '20', 'Twenty.']\n"
     ]
    }
   ],
   "source": [
    "## Lets go more complex \n",
    "print(re.findall('[A-Za-z\\.\\:\\d]+', string_to_be_tokenized)) ## This gives every thing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['SOLDIER', '#1', 'Found', 'them', '?', 'In', 'Mercea', '?', 'The', 'coconut', 's', 'tropical', '!']\n"
     ]
    }
   ],
   "source": [
    "## Task 1: Retain sentence punctuation as separate tokens, but have '#1' remain a single token.\n",
    "my_string = \"SOLDIER #1: Found them? In Mercea? The coconut's tropical!\"\n",
    "pattern = r'(\\w+|#\\d|\\?|!)'\n",
    "print(re.findall(pattern, my_string))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Wann', 'Pizza', 'Und', 'Ã¤hrst', 'Ãœber']\n"
     ]
    }
   ],
   "source": [
    "german_text = 'Wann gehen wir Pizza essen? ðŸ• Und fÃ¤hrst du mit Ãœber? ðŸš•'\n",
    "capital_words = r\"['A-ZÃœÃ¤]\\w+\"\n",
    "print(regexp_tokenize(german_text, capital_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Few Regex Questions \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', 'World', 'This', 'is', 'NLP', 'Welcome', 'to', 'Python']\n"
     ]
    }
   ],
   "source": [
    "text = \"Hello World! This is NLP 101. Welcome to Python\"\n",
    "## Task : using re.findll() to extract only text\n",
    "method = r'[A-Za-z]+'\n",
    "print(re.findall(method,text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Ãœber', 'Ãœbungsheft', 'Heute', 'NLP', 'Python']\n"
     ]
    }
   ],
   "source": [
    "text = \"Ãœber das Ãœbungsheft: Heute lernen wir NLP und Python.\"\n",
    "## Capitalize words with the unicode\n",
    "method = r'\\b[ÃœA-Z][a-zA-ZÃ¼Ã¤ÃŸ]*\\b'\n",
    "print(re.findall(method, text))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Peter', ':', 'Hello', '!', 'How', 'are', 'you', '?'], ['Anna', ':', 'I', \"'m\", 'good', ',', 'thanks', '!', 'And', 'you', '?'], ['Peter', ':', 'Doing', 'great', '.', 'Want', 'to', 'grab', 'some', 'pizza', '?', 'ðŸ•']]\n"
     ]
    }
   ],
   "source": [
    "script = \"\"\"Peter: Hello! How are you?\n",
    "Anna: I'm good, thanks! And you?\n",
    "Peter: Doing great. Want to grab some pizza? ðŸ•\"\"\"\n",
    "\n",
    "## Splitting the whole script into the lines\n",
    "\n",
    "lines = script.split('\\n')\n",
    "tokens = [nltk.word_tokenize(l) for l in lines]\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Peter', 'Hello', 'How', 'are', 'you', 'Anna', 'I', 'm', 'good', 'thanks', 'And', 'you', 'Peter', 'Doing', 'great', 'Want', 'to', 'grab', 'some', 'pizza', 'ðŸ•']\n"
     ]
    }
   ],
   "source": [
    "### Extracting the unique letters only \n",
    "method = r'[A-Za-zÃœ]+|[\\U0001F300-\\U0001F5FF]'\n",
    "print(re.findall(method,script))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to calculate the number of words or the frequency of the words !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 408,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('the', 8), (',', 6), ('a', 5), ('of', 5), ('in', 2), ('town', 2), ('who', 2), ('to', 2), ('.', 2), ('was', 2)]\n"
     ]
    }
   ],
   "source": [
    "### Library to be used \n",
    "from collections import Counter\n",
    "text =\"\"\" In a small town surrounded by rolling hills, the mornings always began with the same rhythm: the smell of fresh bread from the bakery, the sound of bicycles clattering over cobblestones, and the chatter of neighbors who seemed to know every detail of each otherâ€™s lives. Yet, hidden in the routine was a sense of quiet anticipation, as though the town itself was waiting for something unexpected to arriveâ€”like a letter without a return address, or a stranger who carried stories no one had heard before.\"\"\"\n",
    "tokenized_text = word_tokenize(text)\n",
    "lower_tokenized_text = [t.lower() for t in tokenized_text]\n",
    "total_count = Counter(lower_tokenized_text)\n",
    "print(total_count.most_common(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing the text before NLP "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('town', 2), ('small', 1), ('surrounded', 1), ('rolling', 1), ('hill', 1), ('morning', 1), ('always', 1), ('began', 1), ('rhythm', 1), ('smell', 1)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/sujaladhikari/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/sujaladhikari/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     /Users/sujaladhikari/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "## Lets do the preprocessing of the text before NLP \n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "## The first step involves the removal of the non alpha characters\n",
    "\n",
    "only_alpha_characters = [t for t in lower_tokenized_text if t.isalpha()]\n",
    "\n",
    "## Then we remove the english stops such as 'a', 'the', 'and' becuase they donot carry much meaning \n",
    "\n",
    "removal_of_english_characters = [t for t in only_alpha_characters if t not in stopwords.words('english')]\n",
    "\n",
    "## Then we lemmatize or we simply count the two words with the same meaning as one such as mice and mouse is considered mouse\n",
    "\n",
    "word_lemmatizer = WordNetLemmatizer()\n",
    "## Now simply what we can do is lemmantize each word \n",
    "\n",
    "each_lemmantized_word = [word_lemmatizer.lemmatize(t) for t in removal_of_english_characters]\n",
    "\n",
    "## bag of words \n",
    "bow = Counter(each_lemmantized_word)\n",
    "print(bow.most_common(10))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction to genism "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Terms used:\n",
    "### 1. Genism: Genism is the way of making computers understand the text efficiently \n",
    "### 2. Corpus: Corpus is the collection of books, or sentences that computer can read!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('AI', 3), ('healthcare', 2), ('identifying', 2), ('medical', 2), ('Artificial', 1), ('intelligence', 1), ('transforming', 1), ('enhancing', 1), ('diagnostics', 1), ('personalizing', 1)]\n",
      "[(0, 3), (1, 1), (2, 1), (3, 1), (4, 1), (5, 1), (6, 1), (7, 1), (8, 1), (9, 1), (10, 1), (11, 1), (12, 1), (13, 1), (14, 1), (15, 1), (16, 1), (17, 1), (18, 1), (19, 1), (20, 1), (21, 1), (22, 1), (23, 1), (24, 1), (25, 1), (26, 1), (27, 1), (28, 1), (29, 2), (30, 1), (31, 2), (32, 1), (33, 1), (34, 1), (35, 1), (36, 1), (37, 1), (38, 1), (39, 2), (40, 1), (41, 1), (42, 1), (43, 1), (44, 1), (45, 1), (46, 1), (47, 1), (48, 1), (49, 1), (50, 1), (51, 1), (52, 1), (53, 1), (54, 1), (55, 1), (56, 1)]\n"
     ]
    }
   ],
   "source": [
    "from gensim.corpora.dictionary import Dictionary\n",
    "\n",
    "\n",
    "article = \"\"\"Artificial intelligence (AI) is transforming healthcare by enhancing diagnostics, personalizing treatment, and streamlining administrative tasks. Machine learning algorithms can analyze large datasets, identifying patterns that may be missed by humans. AI-powered tools can assist in early disease detection, such as identifying tumors in medical imaging, and can even predict patient outcomes. However, ethical concerns around data privacy, algorithmic bias, and the role of AI in decision-making must be carefully managed. Despite these challenges, the potential benefits of AI in healthcare are immense, promising more accurate, efficient, and accessible medical care.\"\"\"\n",
    "\n",
    "\n",
    "## So now from this article what we can do is simply at first divide it into the tokens\n",
    "tokenized_article = word_tokenize(article)\n",
    "\n",
    "## lets reduce the whole words by replacing the ones with the same meaning as one\n",
    "\n",
    "tokenized_alpha = [t for t in tokenized_article if t.isalpha()] ## Only alphabetical characters\n",
    "\n",
    "## Then we remove the common english characters such as a , the , an \n",
    "\n",
    "common_english_removal = [t for t in tokenized_alpha if t not in stopwords.words('english')]\n",
    "\n",
    "## lemanitizing the word \n",
    "\n",
    "word_lemmatizer = WordNetLemmatizer() \n",
    "\n",
    "each_lemmantized_word = [word_lemmatizer.lemmatize(t) for t in common_english_removal]\n",
    "\n",
    "\n",
    "bow = Counter(each_lemmantized_word)\n",
    "print(bow.most_common(10))\n",
    "\n",
    "## Other way is to use the Dictionary \n",
    "\n",
    "dictionary = Dictionary([each_lemmantized_word])\n",
    "\n",
    "corpus=[dictionary.doc2bow(each_lemmantized_word)]\n",
    "\n",
    "doc = corpus[0]\n",
    "print(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use Counter for human understandable texts\n",
    "### Use the doc2bow or dictionary for the "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction to spaCy\n",
    "### What is spaCy\n",
    "### spaCy is one of the library used to parse the text and also is mostly used due to its rapid speed !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 411,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hey', 'there', 'its', 'me', 'Sujal', 'Adhikari']\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "doc = nlp(\"Hey there its me Sujal Adhikari \")\n",
    "print([token.text for token in doc])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lets take out the detail of one paragraph of elon musk and see how spacy helps to parse it ?! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 412,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPE Pretoria\n",
      "GPE South Africa\n",
      "PERSON Musk\n",
      "DATE 1989\n",
      "GPE Canada\n",
      "ORG the University of Pennsylvania\n",
      "DATE 1997\n",
      "GPE California\n",
      "GPE United States\n",
      "DATE 1995\n",
      "PERSON Musk\n",
      "PRODUCT Zip2\n",
      "DATE 1999\n",
      "ORG PayPal\n",
      "ORG eBay\n",
      "DATE 2002\n",
      "DATE That year\n",
      "PERSON Musk\n",
      "NORP American\n",
      "DATE 2002\n",
      "PERSON Musk\n",
      "PERSON Musk\n",
      "ORG Tesla\n",
      "DATE 2004\n",
      "DATE 2008\n",
      "DATE 2015\n",
      "GPE OpenAI\n",
      "ORG AI\n",
      "ORG AI\n",
      "DATE the 2020s\n",
      "DATE 2022\n",
      "PRODUCT Twitter\n",
      "DATE 2023\n",
      "ORG Neuralink\n",
      "DATE 2016\n",
      "ORG the Boring Company\n",
      "DATE 2017\n"
     ]
    }
   ],
   "source": [
    "paragraph = \"\"\"Born to a wealthy family in Pretoria, South Africa, Musk emigrated in 1989 to Canada. He received bachelor's degrees from the University of Pennsylvania in 1997 before moving to California, United States, to pursue business ventures. In 1995, Musk co-founded the software company Zip2. Following its sale in 1999, he co-founded X.com, an online payment company that later merged to form PayPal, which was acquired by eBay in 2002. That year, Musk also became an American citizen.\n",
    "\n",
    "In 2002, Musk founded the space technology company SpaceX, becoming its CEO and chief engineer; the company has since led innovations in reusable rockets and commercial spaceflight. Musk joined the automaker Tesla as an early investor in 2004 and became its CEO and product architect in 2008; it has since become a leader in electric vehicles. In 2015, he co-founded OpenAI to advance artificial intelligence (AI) research but later left; growing discontent with the organization's direction and their leadership in the AI boom in the 2020s led him to establish xAI. In 2022, he acquired the social network Twitter, implementing significant changes and rebranding it as X in 2023. His other businesses include the neurotechnology company Neuralink, which he co-founded in 2016, and the tunneling company the Boring Company, which he founded in 2017.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "doc = nlp(paragraph)\n",
    "for ent in doc.ents:\n",
    "    print(ent.label_, ent.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We can see how spacy extracts the tokens and fits them in the inbuilt parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Natural Language Processing with spaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We are learning NLP.\n",
      "I am learning NLP with the help of spaCy\n"
     ]
    }
   ],
   "source": [
    "### Sentence Segmentation.\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "text = 'We are learning NLP. I am learning NLP with the help of spaCy'\n",
    "doc = nlp(text)\n",
    "for sent in doc.sents:\n",
    "    print(sent.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 414,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('We', 'we'), ('are', 'be'), ('learning', 'learn'), ('NLP', 'NLP'), ('.', '.'), ('I', 'I'), ('am', 'be'), ('learning', 'learn'), ('NLP', 'NLP'), ('with', 'with'), ('the', 'the'), ('help', 'help'), ('of', 'of'), ('spaCy', 'spaCy')]\n"
     ]
    }
   ],
   "source": [
    "### Lemmatization\n",
    "### lemma is the base from of the token in which the token appears in the dictionary \n",
    "print([(token.text, token.lemma_) for token in doc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 415,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Hey', 'INTJ'], ['there', 'ADV'], ['it', 'PRON'], ['is', 'AUX'], ['me', 'PRON'], ['Sujal', 'PROPN'], ['Adhikari', 'PROPN'], ['.', 'PUNCT'], ['I', 'PRON'], ['am', 'AUX'], ['here', 'ADV'], ['to', 'PART'], ['talk', 'VERB'], ['about', 'ADP'], ['NLP', 'PROPN'], ['.', 'PUNCT'], ['He', 'PRON'], ['is', 'AUX'], ['good', 'ADJ']]\n"
     ]
    }
   ],
   "source": [
    "### Part of Speech \n",
    "### Part of speech is the process of labeling or seperating the whole text into its grammatical forms \n",
    "text = 'Hey there it is me Sujal Adhikari. I am here to talk about NLP. He is good'\n",
    "doc = nlp(text)\n",
    "print([[token.text, token.pos_] for token in doc])\n",
    "\n",
    "## here there is the adverb, it is the pronoun and talk is the verb "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to access the named entities in spacy ?!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DATE Today\n",
      "PERSON Adhikari\n",
      "DATE 2005\n",
      "ORG Tulsi Ram Adhikari\n",
      "PERSON Juna Adhikari\n"
     ]
    }
   ],
   "source": [
    "text = 'Today we will be accessing the named entities in the whole text. For that I will be using example such as Sujal Adhikari. He was born in 2005. He is the yougest son of Tulsi Ram Adhikari and Juna Adhikari. His idol are his parents'\n",
    "\n",
    "## Lets first covert it into the spacy doc \n",
    "doc = nlp(text)\n",
    "for each in doc.ents:\n",
    "    print(each.label_, each.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dependecy Parsing in spaCy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Today -> npadvmod (noun phrase as adverbial modifier) -> accessing\n",
      "we -> nsubj (nominal subject) -> accessing\n",
      "will -> aux (auxiliary) -> accessing\n",
      "be -> aux (auxiliary) -> accessing\n",
      "accessing -> ROOT (root) -> accessing\n",
      "the -> det (determiner) -> entities\n",
      "named -> amod (adjectival modifier) -> entities\n",
      "entities -> dobj (direct object) -> accessing\n",
      "in -> prep (prepositional modifier) -> entities\n",
      "the -> det (determiner) -> text\n",
      "whole -> amod (adjectival modifier) -> text\n",
      "text -> pobj (object of preposition) -> in\n",
      ". -> punct (punctuation) -> accessing\n",
      "For -> prep (prepositional modifier) -> using\n",
      "that -> pobj (object of preposition) -> For\n",
      "I -> nsubj (nominal subject) -> using\n",
      "will -> aux (auxiliary) -> using\n",
      "be -> aux (auxiliary) -> using\n",
      "using -> ROOT (root) -> using\n",
      "example -> dobj (direct object) -> using\n",
      "such -> amod (adjectival modifier) -> as\n",
      "as -> prep (prepositional modifier) -> example\n",
      "Sujal -> compound (compound) -> Adhikari\n",
      "Adhikari -> pobj (object of preposition) -> as\n",
      ". -> punct (punctuation) -> using\n",
      "He -> nsubjpass (nominal subject (passive)) -> born\n",
      "was -> auxpass (auxiliary (passive)) -> born\n",
      "born -> ROOT (root) -> born\n",
      "in -> prep (prepositional modifier) -> born\n",
      "2005 -> pobj (object of preposition) -> in\n",
      ". -> punct (punctuation) -> born\n",
      "He -> nsubj (nominal subject) -> is\n",
      "is -> ROOT (root) -> is\n",
      "the -> det (determiner) -> son\n",
      "yougest -> amod (adjectival modifier) -> son\n",
      "son -> attr (attribute) -> is\n",
      "of -> prep (prepositional modifier) -> son\n",
      "Tulsi -> compound (compound) -> Adhikari\n",
      "Ram -> compound (compound) -> Adhikari\n",
      "Adhikari -> pobj (object of preposition) -> of\n",
      "and -> cc (coordinating conjunction) -> Adhikari\n",
      "Juna -> compound (compound) -> Adhikari\n",
      "Adhikari -> conj (conjunct) -> Adhikari\n",
      ". -> punct (punctuation) -> is\n",
      "His -> poss (possession modifier) -> idol\n",
      "idol -> nsubj (nominal subject) -> are\n",
      "are -> ROOT (root) -> are\n",
      "his -> poss (possession modifier) -> parents\n",
      "parents -> attr (attribute) -> are\n"
     ]
    }
   ],
   "source": [
    "### Dependency parsing is the way of figuring out the grammatical structure of the sentence.\n",
    "\n",
    "for token in doc:\n",
    "    print(f'{token.text} -> {token.dep_} ({spacy.explain(token.dep_)}) -> {token.head.text}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Vectors "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word vector can be defined as the way of represeting the word in terms of math in order to make sure that the computer understands the word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "metadata": {},
   "outputs": [],
   "source": [
    "### To get started with the word vectors we need to import the medium mode of the spacy library \n",
    "nlp = spacy.load('en_core_web_md')\n",
    "## the md library has word vectors compared to sm which has none."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cat 5439657043933447811\n",
      "CAT 14858270728821099472\n"
     ]
    }
   ],
   "source": [
    "## Lexeme ID \n",
    "doc = 'cat CAT'\n",
    "doc = nlp(doc)\n",
    "for each in doc:\n",
    "    print(each.text, each.orth)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### lets figure out the word vector id of each word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 420,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The word vector for Sujal is 3913022940631744379\n",
      "---------------\n",
      "The word vector for Adhikari is 11886398305251611138\n",
      "---------------\n",
      "The word vector for is is 3411606890003347522\n",
      "---------------\n",
      "The word vector for my is 227504873216781231\n",
      "---------------\n",
      "The word vector for name is 18309932012808971453\n",
      "---------------\n"
     ]
    }
   ],
   "source": [
    "text = \"Sujal Adhikari is my name\"\n",
    "doc = nlp(text)\n",
    "for each in doc:\n",
    "    print(f'The word vector for {each} is {nlp.vocab.strings[each.text]}')\n",
    "    print(f'---------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 421,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([[  925154846061690816, 13209368375629525662,  2254214663468316932,\n",
      "        16430854104370637824,  7637407124705328936]], dtype=uint64), array([[6607, 7508, 2766, 1141,  892]], dtype=int32), array([[1.    , 0.93  , 0.9172, 0.8294, 0.8249]], dtype=float32))\n"
     ]
    }
   ],
   "source": [
    "### How to get the similar words for the whole text\n",
    "text = 'covid'\n",
    "vector = nlp(text).vector\n",
    "\n",
    "# Find most similar words\n",
    "similar = nlp.vocab.vectors.most_similar(\n",
    "    vector.reshape(1, -1),  # reshape to (1, dim)\n",
    "    n=5\n",
    ")\n",
    "print(similar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 437,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'[E058] Could not retrieve vector for key 16786716946045152433.'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[437]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m alex = nlp.vocab.vectors.most_similar(nlp.vocab.vectors[nlp.vocab.strings[text]].reshape(\u001b[32m1\u001b[39m,-\u001b[32m1\u001b[39m), n = \u001b[32m5\u001b[39m)\n\u001b[32m      2\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mlen\u001b[39m(alex))\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(alex)):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/nlp_env/lib/python3.11/site-packages/spacy/vectors.pyx:274\u001b[39m, in \u001b[36mspacy.vectors.Vectors.__getitem__\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[31mKeyError\u001b[39m: '[E058] Could not retrieve vector for key 16786716946045152433.'"
     ]
    }
   ],
   "source": [
    "alex = nlp.vocab.vectors.most_similar(nlp.vocab.vectors[nlp.vocab.strings[text]].reshape(1,-1), n = 5)\n",
    "print(len(alex))\n",
    "for i in range(len(alex)):\n",
    "    print(f\"The word is {nlp.vocab.string[i]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key words:\n",
    "### 1. nlp.vocab -> is the way to get access to the vocabulary of the whole spacy data\n",
    "### 2. nlp.vocab.strings -> is the way of getting an ID for a certain word \n",
    "### 3. nlp.vocab.vectors -> is the way of getting the vector Id of the certain word after an ID is created out of it.\n",
    "### 4. nlp.vocab.vectors.most_similar -> is the way of getting the most similar word, it gives list of lists where you can access each word by simply word[0], the shape of the whole vecor should be (1, n_dimension).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The most similar word is amazonin with the similarity score of 1.000\n",
      "The most similar word is MyUS.com with the similarity score of 0.771\n",
      "The most similar word is 7.5W with the similarity score of 0.668\n",
      "The most similar word is NewEgg with the similarity score of 0.630\n",
      "The most similar word is moogle with the similarity score of 0.574\n",
      "The most similar word is Copple with the similarity score of 0.557\n",
      "The most similar word is -Target with the similarity score of 0.552\n",
      "The most similar word is enkindle with the similarity score of 0.541\n",
      "The most similar word is selfridges.com with the similarity score of 0.539\n",
      "The most similar word is Smilers with the similarity score of 0.523\n",
      "The most similar word is waitrose.com with the similarity score of 0.520\n",
      "The most similar word is OMH with the similarity score of 0.520\n",
      "The most similar word is FandangoNow with the similarity score of 0.514\n",
      "The most similar word is RAMALLAH with the similarity score of 0.501\n",
      "The most similar word is fov with the similarity score of 0.498\n",
      "The most similar word is RealtyStore with the similarity score of 0.495\n",
      "The most similar word is SCi with the similarity score of 0.491\n",
      "The most similar word is scoopers with the similarity score of 0.491\n",
      "The most similar word is kwik with the similarity score of 0.482\n",
      "The most similar word is 10y with the similarity score of 0.474\n"
     ]
    }
   ],
   "source": [
    "### Lets print out the most similar words of the word -> \"Amazon\"\n",
    "## Step wise \n",
    "##1. The first step is to convert it to the word vector \n",
    "word = 'Amazon'\n",
    "doc = nlp(word).vector\n",
    "similar_words = nlp.vocab.vectors.most_similar(doc.reshape(1,-1), n =20)\n",
    "keys, rows, scores = similar_words\n",
    "for key, score in zip(keys[0],scores[0]):\n",
    "    print(f\"The most similar word is {nlp.vocab.strings[key]} with the similarity score of {score:.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Semantic Similarity in spacy \n",
    "### Semantic Similarity is the cosine of the two vectors. If the cosine of the two vectors is identical or near to 1 that means the word are similar or kind of similar \n",
    "### To check the semantic similarity we can use .similarity in the doc "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The similarity score of sentence 1 is 0.34898803254918526\n",
      "The similarity score of sentence 2 is 0.2930216053129958\n"
     ]
    }
   ],
   "source": [
    "key = nlp(\"Data Scientist\")\n",
    "sentences = nlp(\"Being a Data Scientist is not easy. It requires tons of Machine Learning knowledge\")\n",
    "for i, sent in enumerate(sentences.sents):\n",
    "    print(f\"The similarity score of sentence {i+1} is {key.similarity(sent)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### spaCy pipelines\n",
    "\n",
    "### spaCy pipelines is the way of converting a random text to computer understandable document or word vector. It involves certain preprocessing steps to deal with."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Steps to create a pipeline\n",
    "### 1. nlp = spacy.blank('en') -> to start a pipeline or to make a pipeline work \n",
    "### 2. nlp.add_pipe('pipeline_name') -> it is used to add the desired component wether be it: sentencizer, or lemmatizer, or parser \n",
    "### 3. nlp.analyze_pipes -> it is used to see what is going on in your pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\n",
      "============================= Pipeline Overview =============================\u001b[0m\n",
      "\n",
      "#   Component    Assigns       Requires   Scores      Retokenizes\n",
      "-   ----------   -----------   --------   ---------   -----------\n",
      "0   lemmatizer   token.lemma              lemma_acc   False      \n",
      "\n",
      "\u001b[38;5;2mâœ” No problems found.\u001b[0m\n",
      "{'summary': {'lemmatizer': {'assigns': ['token.lemma'], 'requires': [], 'scores': ['lemma_acc'], 'retokenizes': False}}, 'problems': {'lemmatizer': []}, 'attrs': {'token.lemma': {'assigns': ['lemmatizer'], 'requires': []}}}\n"
     ]
    }
   ],
   "source": [
    "### Creating our won spacy pipeline\n",
    "pipeline = spacy.blank('en')\n",
    "adding_lemmatizer = pipeline.add_pipe('lemmatizer')\n",
    "\n",
    "print(pipeline.analyze_pipes(pretty=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entity ruler in Spacy !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 424,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PERSON Sujal\n",
      "ORG JP Morgan Chase\n"
     ]
    }
   ],
   "source": [
    "### Entity ruler in spacy is the way of adding your own entities in the pipeline. For example in the text\n",
    "from spacy.pipeline import EntityRuler\n",
    "text = 'Sujal wants to work in JP Morgan Chase'\n",
    "doc = nlp(text)\n",
    "for each in doc.ents:\n",
    "    print(each.label_, each.text) ## here the person in Sujal and JP Morgan Chase is org, we can change that add put our default.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 426,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Sujal', 'HUMAN'), ('Morgan Chase', 'ORG')]\n"
     ]
    }
   ],
   "source": [
    "### Lets create a pipeline and also a pattern using the entity ruler method \n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "ruler = nlp.add_pipe('entity_ruler',before = 'ner')\n",
    "\n",
    "### Lets create custom pattern \n",
    "pattern = [\n",
    "    {\"label\":'HUMAN','pattern':'Sujal' },\n",
    "]\n",
    "ruler.add_patterns(pattern)\n",
    "\n",
    "doc = nlp('Sujal works in JP Morgan Chase')\n",
    "print([(ent.text, ent.label_) for ent in doc.ents])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### When to use before and after in ner \n",
    "### Use before -> when you let model make the decisions even though you have a pattern\n",
    "### Use after -> If you trust your pattern more than the model's default pattern.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Token based patterns \n",
    "### It is the way of cutomizing your own tokens in order to make the model return and recognize the entity based on your pattern \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 436,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Mr. Binod Chaudary', 'PERSON'), ('the Wai Wai Noodels Factory', 'ORG')]\n"
     ]
    }
   ],
   "source": [
    "## Lets create own custom pattern s\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "ruler = nlp.add_pipe('entity_ruler', before = 'ner')\n",
    "pattern = [\n",
    "    {'label':'ORG', 'pattern':[{'TEXT':'Wai Wai Noodles Factory'}]},\n",
    "    {'label':'PERSON', 'pattern':[{'LOWER':'mr.'}, {'IS_TITLE':True},{'IS_TITLE':True}]} ## To match the first name and the last name.\n",
    "]\n",
    "ruler.add_patterns(pattern)\n",
    "text = 'Mr. Binod Chaudary is the founder of the Wai Wai Noodels Factory'\n",
    "print([(ent.text, ent.label_) for ent in nlp(text).ents])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 454,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "### lets calculate the number or extract the number from the given text\n",
    "### We will be building the spacy model that takes the text from any of the online platform and then extractst he number be it any format: \n",
    "text = input(\"Enter the text\")\n",
    "spacyModel = spacy.blank('en')\n",
    "ruler = spacyModel.add_pipe('entity_ruler')\n",
    "pattern = [{'label':'PHONE_NUMBER', 'pattern':[{\"TEXT\": {\"REGEX\": r\"\\((\\d){3}\\)-(\\d){3}-(\\d){4}|\\d{10}|\\((\\d){3}\\) (\\d){3} (\\d){4}\"}}]}]\n",
    "ruler.add_patterns(pattern)\n",
    "doc = spacyModel(text)\n",
    "print([(ent.text,ent.label_) for ent in doc.ents])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### spaCy Matcher and Parsher \n",
    "\n",
    "### spaCy Matcher can be taken as the search engine for the whole nlp document!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 457,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SuJaL AdHIKAri\n"
     ]
    }
   ],
   "source": [
    "from spacy.matcher import Matcher \n",
    "nlp = spacy.blank('en')\n",
    "matcher = Matcher(nlp.vocab)\n",
    "pattern = [{'LOWER':'sujal'},{'LOWER':'adhikari'}]\n",
    "doc = nlp(\"SuJaL AdHIKAri is the man.\")\n",
    "matcher.add(\"NAME\", [pattern])\n",
    "matches = matcher(doc)\n",
    "for matcher_id, start, end in matches:\n",
    "    span = doc[start:end]\n",
    "    print(span.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Phrase Matcher \n",
    "### Phrase Matcher is the way of matching the phrase instead of manually matching the whole text\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 466,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The names of the family members are: \n",
      " Sujal Adhikari\n",
      "The names of the family members are: \n",
      " Tulsi Ram Adhikari\n",
      "The names of the family members are: \n",
      " Juna Adhikari\n"
     ]
    }
   ],
   "source": [
    "from spacy.matcher import PhraseMatcher\n",
    "text = 'Sujal Adhikari is the youngest son to Tulsi Ram Adhikari and Juna Adhikari'\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(text)\n",
    "phrases = ['SUJAL ADHIKARI', 'JUNA ADHIKARI', 'TULSI RAM ADHIKARI']\n",
    "matcher = PhraseMatcher(nlp.vocab, attr = 'LOWER')\n",
    "patterns = [nlp.make_doc(match) for match in phrases]\n",
    "matcher.add('NAME ', patterns)\n",
    "\n",
    "matches = matcher(doc)\n",
    "\n",
    "for matcher_id, start, end in matches:\n",
    "    print(f'The names of the family members are: \\n {doc[start:end].text}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.12.7",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
