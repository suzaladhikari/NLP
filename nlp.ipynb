{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regular Expressions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regular expressions helps us to find the pattern in the given string which helps to tackle the classification problems\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<re.Match object; span=(0, 5), match='sujal'>\n"
     ]
    }
   ],
   "source": [
    "mystring = 'sujal'\n",
    "print(re.match(mystring, 'sujal adhikari is the man')) \n",
    "## Here the match is sujal or sujal is the common between the two strings \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<re.Match object; span=(0, 5), match='Peter'>\n"
     ]
    }
   ],
   "source": [
    "used_regex = '\\w+'\n",
    "print(re.match(used_regex, \"Peter Fernandez\"))\n",
    "## Here only peter is printed becuase the space is not the string or the character"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Common regex patterns \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "## There are tons of patterns in the regex but the most common are \n",
    "#print(re.match('\\w+', 'Sujal Adhikari ')) ## Gives all the strings before the character that is not space\n",
    "print(re.match('\\d+', 'DOB 2005'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization is the process of breaking down the strings into chunks in order to make it easy for the strings "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hey', 'there', 'fellas', 'Sujal', 'Adhikari', 'here']\n"
     ]
    }
   ],
   "source": [
    "## Using the normal regex library \n",
    "my_string = \"Hey there fellas ! Sujal Adhikari here!\"\n",
    "print(re.findall(r'\\w+', my_string)) ## So basically, it turns the whole sentence into the tokens \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regex or regular expressions only work on strings "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NLTK -> Natural Language Toolkit \n",
    "### It is one of the handy library which is used to do tokenization, stemming, lemmatization, parsing and many more "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<re.Match object; span=(19, 29), match='8627038353'>\n"
     ]
    }
   ],
   "source": [
    "number = \"My phone number is 8627038353\"\n",
    "expression = r'\\d{10}'\n",
    "print(re.search(expression,number)) ## The output is the number "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'admires', 'He', 'the', 'who', 'Peter', '.', 'everyone', 'is', 'man'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /Users/sujaladhikari/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize,regexp_tokenize\n",
    "scene_one = 'Peter is the man who everyone admires. He is the man'\n",
    "nltk.download('punkt_tab') \n",
    "# Split scene_one into sentences: sentences\n",
    "sentences = sent_tokenize(scene_one)\n",
    "\n",
    "# Use word_tokenize to tokenize the fourth sentence: tokenized_sent\n",
    "tokenized_sent = word_tokenize(sentences[1])\n",
    "\n",
    "# Make a set of unique tokens in the entire scene: unique_tokens\n",
    "unique_tokens = set(word_tokenize(scene_one))\n",
    "\n",
    "# Print the unique tokens result\n",
    "print(unique_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regex groupping using the letter '|'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['He', 'has', '11', 'cats']\n"
     ]
    }
   ],
   "source": [
    "match_digits_and_words = ('(\\d+|\\w+)')\n",
    "print(re.findall(match_digits_and_words, 'He has 11 cats'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some useful tokenization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['NAME', 'SUJAL', 'ADHIKARI', 'Age', 'Twenty']\n"
     ]
    }
   ],
   "source": [
    "string_to_be_tokenized = 'NAME: SUJAL ADHIKARI, Age: 20, Twenty.'\n",
    "print(re.findall('[A-Za-z]+',string_to_be_tokenized)) ## These are the ones with the upper and lower case in it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['NAME:', 'SUJAL', 'ADHIKARI', 'Age:', '20', 'Twenty.']\n"
     ]
    }
   ],
   "source": [
    "## Lets go more complex \n",
    "print(re.findall('[A-Za-z\\.\\:\\d]+', string_to_be_tokenized)) ## This gives every thing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['SOLDIER', '#1', 'Found', 'them', '?', 'In', 'Mercea', '?', 'The', 'coconut', 's', 'tropical', '!']\n"
     ]
    }
   ],
   "source": [
    "## Task 1: Retain sentence punctuation as separate tokens, but have '#1' remain a single token.\n",
    "my_string = \"SOLDIER #1: Found them? In Mercea? The coconut's tropical!\"\n",
    "pattern = r'(\\w+|#\\d|\\?|!)'\n",
    "print(re.findall(pattern, my_string))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Wann', 'Pizza', 'Und', '√§hrst', '√úber']\n"
     ]
    }
   ],
   "source": [
    "german_text = 'Wann gehen wir Pizza essen? üçï Und f√§hrst du mit √úber? üöï'\n",
    "capital_words = r\"['A-Z√ú√§]\\w+\"\n",
    "print(regexp_tokenize(german_text, capital_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Few Regex Questions \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', 'World', 'This', 'is', 'NLP', 'Welcome', 'to', 'Python']\n"
     ]
    }
   ],
   "source": [
    "text = \"Hello World! This is NLP 101. Welcome to Python\"\n",
    "## Task : using re.findll() to extract only text\n",
    "method = r'[A-Za-z]+'\n",
    "print(re.findall(method,text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['√úber', '√úbungsheft', 'Heute', 'NLP', 'Python']\n"
     ]
    }
   ],
   "source": [
    "text = \"√úber das √úbungsheft: Heute lernen wir NLP und Python.\"\n",
    "## Capitalize words with the unicode\n",
    "method = r'\\b[√úA-Z][a-zA-Z√º√§√ü]*\\b'\n",
    "print(re.findall(method, text))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Peter', ':', 'Hello', '!', 'How', 'are', 'you', '?'], ['Anna', ':', 'I', \"'m\", 'good', ',', 'thanks', '!', 'And', 'you', '?'], ['Peter', ':', 'Doing', 'great', '.', 'Want', 'to', 'grab', 'some', 'pizza', '?', 'üçï']]\n"
     ]
    }
   ],
   "source": [
    "script = \"\"\"Peter: Hello! How are you?\n",
    "Anna: I'm good, thanks! And you?\n",
    "Peter: Doing great. Want to grab some pizza? üçï\"\"\"\n",
    "\n",
    "## Splitting the whole script into the lines\n",
    "\n",
    "lines = script.split('\\n')\n",
    "tokens = [nltk.word_tokenize(l) for l in lines]\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Peter', 'Hello', 'How', 'are', 'you', 'Anna', 'I', 'm', 'good', 'thanks', 'And', 'you', 'Peter', 'Doing', 'great', 'Want', 'to', 'grab', 'some', 'pizza', 'üçï']\n"
     ]
    }
   ],
   "source": [
    "### Extracting the unique letters only \n",
    "method = r'[A-Za-z√ú]+|[\\U0001F300-\\U0001F5FF]'\n",
    "print(re.findall(method,script))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to calculate the number of words or the frequency of the words !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('the', 8), (',', 6), ('a', 5), ('of', 5), ('in', 2), ('town', 2), ('who', 2), ('to', 2), ('.', 2), ('was', 2)]\n"
     ]
    }
   ],
   "source": [
    "### Library to be used \n",
    "from collections import Counter\n",
    "text =\"\"\" In a small town surrounded by rolling hills, the mornings always began with the same rhythm: the smell of fresh bread from the bakery, the sound of bicycles clattering over cobblestones, and the chatter of neighbors who seemed to know every detail of each other‚Äôs lives. Yet, hidden in the routine was a sense of quiet anticipation, as though the town itself was waiting for something unexpected to arrive‚Äîlike a letter without a return address, or a stranger who carried stories no one had heard before.\"\"\"\n",
    "tokenized_text = word_tokenize(text)\n",
    "lower_tokenized_text = [t.lower() for t in tokenized_text]\n",
    "total_count = Counter(lower_tokenized_text)\n",
    "print(total_count.most_common(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing the text before NLP "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('town', 2), ('small', 1), ('surrounded', 1), ('rolling', 1), ('hill', 1), ('morning', 1), ('always', 1), ('began', 1), ('rhythm', 1), ('smell', 1)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/sujaladhikari/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/sujaladhikari/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     /Users/sujaladhikari/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "## Lets do the preprocessing of the text before NLP \n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "## The first step involves the removal of the non alpha characters\n",
    "\n",
    "only_alpha_characters = [t for t in lower_tokenized_text if t.isalpha()]\n",
    "\n",
    "## Then we remove the english stops such as 'a', 'the', 'and' becuase they donot carry much meaning \n",
    "\n",
    "removal_of_english_characters = [t for t in only_alpha_characters if t not in stopwords.words('english')]\n",
    "\n",
    "## Then we lemmatize or we simply count the two words with the same meaning as one such as mice and mouse is considered mouse\n",
    "\n",
    "word_lemmatizer = WordNetLemmatizer()\n",
    "## Now simply what we can do is lemmantize each word \n",
    "\n",
    "each_lemmantized_word = [word_lemmatizer.lemmatize(t) for t in removal_of_english_characters]\n",
    "\n",
    "## bag of words \n",
    "bow = Counter(each_lemmantized_word)\n",
    "print(bow.most_common(10))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction to genism "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Terms used:\n",
    "### 1. Genism: Genism is the way of making computers understand the text efficiently \n",
    "### 2. Corpus: Corpus is the collection of books, or sentences that computer can read!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('AI', 3), ('healthcare', 2), ('identifying', 2), ('medical', 2), ('Artificial', 1), ('intelligence', 1), ('transforming', 1), ('enhancing', 1), ('diagnostics', 1), ('personalizing', 1)]\n",
      "[(0, 3), (1, 1), (2, 1), (3, 1), (4, 1), (5, 1), (6, 1), (7, 1), (8, 1), (9, 1), (10, 1), (11, 1), (12, 1), (13, 1), (14, 1), (15, 1), (16, 1), (17, 1), (18, 1), (19, 1), (20, 1), (21, 1), (22, 1), (23, 1), (24, 1), (25, 1), (26, 1), (27, 1), (28, 1), (29, 2), (30, 1), (31, 2), (32, 1), (33, 1), (34, 1), (35, 1), (36, 1), (37, 1), (38, 1), (39, 2), (40, 1), (41, 1), (42, 1), (43, 1), (44, 1), (45, 1), (46, 1), (47, 1), (48, 1), (49, 1), (50, 1), (51, 1), (52, 1), (53, 1), (54, 1), (55, 1), (56, 1)]\n"
     ]
    }
   ],
   "source": [
    "from gensim.corpora.dictionary import Dictionary\n",
    "\n",
    "\n",
    "article = \"\"\"Artificial intelligence (AI) is transforming healthcare by enhancing diagnostics, personalizing treatment, and streamlining administrative tasks. Machine learning algorithms can analyze large datasets, identifying patterns that may be missed by humans. AI-powered tools can assist in early disease detection, such as identifying tumors in medical imaging, and can even predict patient outcomes. However, ethical concerns around data privacy, algorithmic bias, and the role of AI in decision-making must be carefully managed. Despite these challenges, the potential benefits of AI in healthcare are immense, promising more accurate, efficient, and accessible medical care.\"\"\"\n",
    "\n",
    "\n",
    "## So now from this article what we can do is simply at first divide it into the tokens\n",
    "tokenized_article = word_tokenize(article)\n",
    "\n",
    "## lets reduce the whole words by replacing the ones with the same meaning as one\n",
    "\n",
    "tokenized_alpha = [t for t in tokenized_article if t.isalpha()] ## Only alphabetical characters\n",
    "\n",
    "## Then we remove the common english characters such as a , the , an \n",
    "\n",
    "common_english_removal = [t for t in tokenized_alpha if t not in stopwords.words('english')]\n",
    "\n",
    "## lemanitizing the word \n",
    "\n",
    "word_lemmatizer = WordNetLemmatizer() \n",
    "\n",
    "each_lemmantized_word = [word_lemmatizer.lemmatize(t) for t in common_english_removal]\n",
    "\n",
    "\n",
    "bow = Counter(each_lemmantized_word)\n",
    "print(bow.most_common(10))\n",
    "\n",
    "## Other way is to use the Dictionary \n",
    "\n",
    "dictionary = Dictionary([each_lemmantized_word])\n",
    "\n",
    "corpus=[dictionary.doc2bow(each_lemmantized_word)]\n",
    "\n",
    "doc = corpus[0]\n",
    "print(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use Counter for human understandable texts\n",
    "### Use the doc2bow or dictionary for the "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TfIdf Vectorizer\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lets use the tfidf vectorizer in one random article of the Wikipedia and then figure out the weight of the article and also how much topic oriented the article is ! \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['So', 'is', 'this', 'swearing', 'or', 'no', 'swearing'], ['‚Äù', 'In', 'a', 'darkened', 'soundstage', 'on', 'the', 'outskirts', 'of', 'London', ',', 'Abel', 'Tesfaye', 'is', 'wondering', 'if', 'he', 'can', 'say', '‚Äú', 'fuck', '‚Äù', 'or', 'not'], ['Tesfaye', ',', 'better', 'known', 'as', 'breakout', 'pop', 'sensation', 'the', 'Weeknd', ',', 'is', 'at', 'a', 'rehearsal', 'for', 'Later'], ['With', 'Jools', 'Holland', ',', 'the', 'BBC', 'music', 'show', ',', 'about', 'to', 'soundcheck', 'his', 'smash', 'hit', '‚Äú', 'The', 'Hills', ',', '‚Äù', 'a', 'four-minute', 'horror-movie', 'booty', 'call', 'featuring', 'more', 'than', 'a', 'dozen', 'f-bombs'], ['For', 'Tesfaye', ',', 'that', '‚Äô', 's', 'relatively', 'clean', ',', 'but', 'he', 'knows', 'the', 'pensioners', 'in', 'Twickenham', 'might', 'disagree'], ['So', 'when', 'the', 'verdict', 'comes', 'back', '‚Äú', 'no', 'swearing', ',', '‚Äù', 'he', 'nods', 'and', 'smoothly', 'pivots', 'to', 'a', 'censored', 'version', '‚Äî', 'a', 'small', 'gesture', 'that', 'says', 'a', 'lot', 'about', 'the', 'kind', 'of', 'professional', 'he', 'has', 'become'], ['‚Äú', 'The', 'Hills', '‚Äù', 'is', 'currently', 'enjoying', 'its', 'fourth', 'straight', 'week', 'at', 'Number', 'One', ',', 'a', 'feat', 'made', 'even', 'more', 'impressive', 'because', 'it', 'took', 'the', 'place', 'of', 'another', 'Weeknd', 'track', ',', '‚Äú', 'Can', '‚Äô', 't', 'Feel', 'My', 'Face', '‚Äù', '‚Äî', 'Spotify', '‚Äô', 's', 'official', 'song', 'of', 'the', 'summer', ',', 'and', 'the', 'only', 'song', 'about', 'cocaine', 'ever', 'to', 'be', 'lip-synced', 'by', 'Tom', 'Cruise', 'on', 'network', 'TV'], ['Tesfaye', 'is', 'just', 'the', '12th', 'artist', 'in', 'history', 'to', 'score', 'back-to-back', 'Number', 'Ones', ',', 'a', 'group', 'that', 'includes', 'Elvis', 'Presley', ',', 'the', 'Beatles', 'and', 'Taylor', 'Swift'], ['His', 'new', 'album', ',', 'Beauty', 'Behind', 'the', 'Madness', ',', 'has', 'sold', 'more', 'than', 'half', 'a', 'million', 'copies', 'in', 'a', 'couple', 'of', 'months', ',', 'and', 'he', '‚Äô', 's', 'preparing', 'to', 'launch', 'a', 'national', 'arena', 'tour', 'in', 'November'], ['‚Äú', 'I', '‚Äô', 'm', 'still', 'digesting', 'it', ',', 'to', 'be', 'honest', 'with', 'you', ',', '‚Äù', 'Tesfaye', 'says', 'of', 'his', 'success'], ['‚Äú', 'But', 'the', 'screams', 'keep', 'getting', 'louder', ',', 'dude'], ['‚Äù', 'Tesfaye', 'comes', 'over', 'to', 'say', 'hi', ',', 'dressed', 'in', 'black', 'Levi', '‚Äô', 's', 'and', 'a', 'Roots', 'hoodie', ',', 'his', 'tsunami', 'of', 'hair', 'piled', 'high', 'atop', 'his', 'head'], ['‚Äú', 'Sorry', ',', 'I', '‚Äô', 'm', 'sick', ',', '‚Äù', 'he', 'says', ',', 'as', 'his', 'handshake', 'becomes', 'a', 'fist', 'bump', 'in', 'midair'], ['Since', 'starting', 'this', 'promo', 'tour', 'a', 'week', 'ago', ',', 'he', '‚Äô', 's', 'been', 'to', 'Las', 'Vegas', ',', 'Paris', ',', 'Berlin', 'and', 'now', 'London'], ['The', 'cold', 'caught', 'up', 'with', 'him', 'yesterday', ',', 'during', 'a', 'signing', 'for', '500', 'squealing', 'fans', 'at', 'the', 'Oxford', 'Circus', 'HMV'], ['(', 'Overheard', ':', '‚Äú', 'I', 'wanted', 'to', 'hug', 'him'], ['‚Äù', '‚Äú', 'You', 'didn', '‚Äô', 't', 'hug', 'him'], ['I', 'kissed', 'him'], ['‚Äù', ')', 'Go', 'behind', 'the', 'scenes', 'of', 'the', 'Weeknd', '‚Äô', 's', 'performance', 'on', 'Saturday', 'Night', 'Live', 'in', 'our', 'exclusive', 'mini-documentary', 'below', ':', 'This', 'scene', 'would', 'not', 'have', 'seemed', 'possible', 'in', '2011', ',', 'when', 'the', 'Weeknd', 'appeared', 'with', 'a', 'trio', 'of', 'cult-favorite', 'mixtapes', 'that', 'established', 'both', 'his', 'sonic', 'template', '‚Äî', 'drug-drenched', ',', 'indie-rock-sampling', ',', 'sex-dungeon', 'R', '&', 'B', '‚Äî', 'and', 'his', 'mysterious', ',', 'brooding', 'persona'], ['A', 'press-shy', 'Ethiopian', 'kid', 'from', 'Toronto', 'who', 'has', 'given', 'only', 'a', 'handful', 'of', 'interviews', ',', 'he', 'has', 'cultivated', 'a', 'near-mythical', 'image', 'as', 'a', 'bed-hopping', ',', 'pill-popping', ',', 'chart-topping', 'cipher'], ['‚Äú', 'We', 'live', 'in', 'an', 'era', 'when', 'everything', 'is', 'so', 'excessive', ',', 'I', 'think', 'it', '‚Äô', 's', 'refreshing', 'for', 'everybody', 'to', 'be', 'like', ',', '‚Äò', 'Who', 'the', 'fuck', 'is', 'this', 'guy'], [\"'\", '‚Äù', 'Tesfaye', 'says'], ['‚Äú', 'I', 'think', 'that', '‚Äô', 's', 'why', 'my', 'career', 'is', 'going', 'to', 'be', 'so', 'long', ':', 'Because', 'I', 'haven', '‚Äô', 't', 'given', 'people', 'everything'], ['‚Äù']]\n",
      "[['So', 'swearing', 'swearing'], ['In', 'darkened', 'soundstage', 'outskirts', 'London', 'Abel', 'Tesfaye', 'wondering', 'say', 'fuck'], ['Tesfaye', 'better', 'known', 'breakout', 'pop', 'sensation', 'Weeknd', 'rehearsal', 'Later'], ['With', 'Jools', 'Holland', 'BBC', 'music', 'show', 'soundcheck', 'smash', 'hit', 'The', 'Hills', 'booty', 'call', 'featuring', 'dozen'], ['For', 'Tesfaye', 'relatively', 'clean', 'knows', 'pensioners', 'Twickenham', 'might', 'disagree'], ['So', 'verdict', 'comes', 'back', 'swearing', 'nods', 'smoothly', 'pivots', 'censored', 'version', 'small', 'gesture', 'says', 'lot', 'kind', 'professional', 'become'], ['The', 'Hills', 'currently', 'enjoying', 'fourth', 'straight', 'week', 'Number', 'One', 'feat', 'made', 'even', 'impressive', 'took', 'place', 'another', 'Weeknd', 'track', 'Can', 'Feel', 'My', 'Face', 'Spotify', 'official', 'song', 'summer', 'song', 'cocaine', 'ever', 'Tom', 'Cruise', 'network', 'TV'], ['Tesfaye', 'artist', 'history', 'score', 'Number', 'Ones', 'group', 'includes', 'Elvis', 'Presley', 'Beatles', 'Taylor', 'Swift'], ['His', 'new', 'album', 'Beauty', 'Behind', 'Madness', 'sold', 'half', 'million', 'copies', 'couple', 'months', 'preparing', 'launch', 'national', 'arena', 'tour', 'November'], ['I', 'still', 'digesting', 'honest', 'Tesfaye', 'says', 'success'], ['But', 'screams', 'keep', 'getting', 'louder', 'dude'], ['Tesfaye', 'comes', 'say', 'hi', 'dressed', 'black', 'Levi', 'Roots', 'hoodie', 'tsunami', 'hair', 'piled', 'high', 'atop', 'head'], ['Sorry', 'I', 'sick', 'says', 'handshake', 'becomes', 'fist', 'bump', 'midair'], ['Since', 'starting', 'promo', 'tour', 'week', 'ago', 'Las', 'Vegas', 'Paris', 'Berlin', 'London'], ['The', 'cold', 'caught', 'yesterday', 'signing', 'squealing', 'fans', 'Oxford', 'Circus', 'HMV'], ['Overheard', 'I', 'wanted', 'hug'], ['You', 'hug'], ['I', 'kissed'], ['Go', 'behind', 'scenes', 'Weeknd', 'performance', 'Saturday', 'Night', 'Live', 'exclusive', 'This', 'scene', 'would', 'seemed', 'possible', 'Weeknd', 'appeared', 'trio', 'mixtapes', 'established', 'sonic', 'template', 'R', 'B', 'mysterious', 'brooding', 'persona'], ['A', 'Ethiopian', 'kid', 'Toronto', 'given', 'handful', 'interviews', 'cultivated', 'image', 'cipher'], ['We', 'live', 'era', 'everything', 'excessive', 'I', 'think', 'refreshing', 'everybody', 'like', 'Who', 'fuck', 'guy'], ['Tesfaye', 'says'], ['I', 'think', 'career', 'going', 'long', 'Because', 'I', 'given', 'people', 'everything'], []]\n",
      "[['So', 'swearing', 'swearing'], ['In', 'darkened', 'soundstage', 'outskirt', 'London', 'Abel', 'Tesfaye', 'wondering', 'say', 'fuck'], ['Tesfaye', 'better', 'known', 'breakout', 'pop', 'sensation', 'Weeknd', 'rehearsal', 'Later'], ['With', 'Jools', 'Holland', 'BBC', 'music', 'show', 'soundcheck', 'smash', 'hit', 'The', 'Hills', 'booty', 'call', 'featuring', 'dozen'], ['For', 'Tesfaye', 'relatively', 'clean', 'know', 'pensioner', 'Twickenham', 'might', 'disagree'], ['So', 'verdict', 'come', 'back', 'swearing', 'nod', 'smoothly', 'pivot', 'censored', 'version', 'small', 'gesture', 'say', 'lot', 'kind', 'professional', 'become'], ['The', 'Hills', 'currently', 'enjoying', 'fourth', 'straight', 'week', 'Number', 'One', 'feat', 'made', 'even', 'impressive', 'took', 'place', 'another', 'Weeknd', 'track', 'Can', 'Feel', 'My', 'Face', 'Spotify', 'official', 'song', 'summer', 'song', 'cocaine', 'ever', 'Tom', 'Cruise', 'network', 'TV'], ['Tesfaye', 'artist', 'history', 'score', 'Number', 'Ones', 'group', 'includes', 'Elvis', 'Presley', 'Beatles', 'Taylor', 'Swift'], ['His', 'new', 'album', 'Beauty', 'Behind', 'Madness', 'sold', 'half', 'million', 'copy', 'couple', 'month', 'preparing', 'launch', 'national', 'arena', 'tour', 'November'], ['I', 'still', 'digesting', 'honest', 'Tesfaye', 'say', 'success'], ['But', 'scream', 'keep', 'getting', 'louder', 'dude'], ['Tesfaye', 'come', 'say', 'hi', 'dressed', 'black', 'Levi', 'Roots', 'hoodie', 'tsunami', 'hair', 'piled', 'high', 'atop', 'head'], ['Sorry', 'I', 'sick', 'say', 'handshake', 'becomes', 'fist', 'bump', 'midair'], ['Since', 'starting', 'promo', 'tour', 'week', 'ago', 'Las', 'Vegas', 'Paris', 'Berlin', 'London'], ['The', 'cold', 'caught', 'yesterday', 'signing', 'squealing', 'fan', 'Oxford', 'Circus', 'HMV'], ['Overheard', 'I', 'wanted', 'hug'], ['You', 'hug'], ['I', 'kissed'], ['Go', 'behind', 'scene', 'Weeknd', 'performance', 'Saturday', 'Night', 'Live', 'exclusive', 'This', 'scene', 'would', 'seemed', 'possible', 'Weeknd', 'appeared', 'trio', 'mixtapes', 'established', 'sonic', 'template', 'R', 'B', 'mysterious', 'brooding', 'persona'], ['A', 'Ethiopian', 'kid', 'Toronto', 'given', 'handful', 'interview', 'cultivated', 'image', 'cipher'], ['We', 'live', 'era', 'everything', 'excessive', 'I', 'think', 'refreshing', 'everybody', 'like', 'Who', 'fuck', 'guy'], ['Tesfaye', 'say'], ['I', 'think', 'career', 'going', 'long', 'Because', 'I', 'given', 'people', 'everything'], []]\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import TfidfModel\n",
    "\n",
    "random_article = \"\"\"So is this swearing or no swearing?‚Äù In a darkened soundstage on the outskirts of London, Abel Tesfaye is wondering if he can say ‚Äúfuck‚Äù or not. Tesfaye, better known as breakout pop sensation the Weeknd, is at a rehearsal for Later...With Jools Holland, the BBC music show, about to soundcheck his smash hit ‚ÄúThe Hills,‚Äù a four-minute horror-movie booty call featuring more than a dozen f-bombs. For Tesfaye, that‚Äôs relatively clean, but he knows the pensioners in Twickenham might disagree. So when the verdict comes back ‚Äúno swearing,‚Äù he nods and smoothly pivots to a censored version ‚Äî a small gesture that says a lot about the kind of professional he has become.\n",
    "\n",
    "‚ÄúThe Hills‚Äù is currently enjoying its fourth straight week at Number One, a feat made even more impressive because it took the place of another Weeknd track, ‚ÄúCan‚Äôt Feel My Face‚Äù ‚Äî Spotify‚Äôs official song of the summer, and the only song about cocaine ever to be lip-synced by Tom Cruise on network TV. Tesfaye is just the 12th artist in history to score back-to-back Number Ones, a group that includes Elvis Presley, the Beatles and Taylor Swift. His new album, Beauty Behind the Madness, has sold more than half a million copies in a couple of months, and he‚Äôs preparing to launch a national arena tour in November. ‚ÄúI‚Äôm still digesting it, to be honest with you,‚Äù Tesfaye says of his success. ‚ÄúBut the screams keep getting louder, dude.‚Äù\n",
    "\n",
    "\n",
    "Tesfaye comes over to say hi, dressed in black Levi‚Äôs and a Roots hoodie, his tsunami of hair piled high atop his head. ‚ÄúSorry, I‚Äôm sick,‚Äù he says, as his handshake becomes a fist bump in midair. Since starting this promo tour a week ago, he‚Äôs been to Las Vegas, Paris, Berlin and now London. The cold caught up with him yesterday, during a signing for 500 squealing fans at the Oxford Circus HMV. (Overheard: ‚ÄúI wanted to hug him!‚Äù ‚ÄúYou didn‚Äôt hug him? I kissed him!‚Äù)\n",
    "\n",
    "\n",
    "Go behind the scenes of the Weeknd‚Äôs performance on Saturday Night Live in our exclusive mini-documentary below:\n",
    "\n",
    "This scene would not have seemed possible in 2011, when the Weeknd appeared with a trio of cult-favorite mixtapes that established both his sonic template ‚Äî drug-drenched, indie-rock-sampling, sex-dungeon R&B ‚Äî and his mysterious, brooding persona. A press-shy Ethiopian kid from Toronto who has given only a handful of interviews, he has cultivated a near-mythical image as a bed-hopping, pill-popping, chart-topping cipher. ‚ÄúWe live in an era when everything is so excessive, I think it‚Äôs refreshing for everybody to be like, ‚ÄòWho the fuck is this guy?'‚Äù Tesfaye says. ‚ÄúI think that‚Äôs why my career is going to be so long: Because I haven‚Äôt given people everything.‚Äù\"\"\" ## this is the article we will be using \n",
    "\n",
    "\n",
    "sentences = re.split(r'[.?!]', random_article)\n",
    "sentences = [s.strip() for s in sentences if s.strip() != \"\"]\n",
    "## at first lets tokenize the whole article\n",
    "\n",
    "tokenized_article = [word_tokenize(t) for t in sentences]\n",
    "\n",
    "\n",
    "## Lets remove non alphabetical characters\n",
    "\n",
    "refined_tokenized_article = [[t for t in each if t.isalpha()] for each in tokenized_article]\n",
    "\n",
    "\n",
    "## Lets remove the common english words such as no, or, the, and , an, a\n",
    "\n",
    "removal_of_english_characters = [[t for t in each_refined if t not in stopwords.words('english')] for each_refined in refined_tokenized_article]\n",
    "\n",
    "\n",
    "\n",
    "## word lemmatizer \n",
    "\n",
    "word_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "lemmatized_words = [[word_lemmatizer.lemmatize(t) for t in each] for each in removal_of_english_characters]\n",
    "\n",
    "\n",
    "## Now lets count the total occurance using the dictionary \n",
    "\n",
    "dictionary = Dictionary(lemmatized_words)\n",
    "\n",
    "### lets give it id and make it bag of words\n",
    "\n",
    "corpus = [dictionary.doc2bow(sentence) for sentence in lemmatized_words]\n",
    "tfidf = TfidfModel(corpus)\n",
    "tfidf_weights = tfidf[corpus[23]]\n",
    "\n",
    "for word_id, weight in tfidf_weights:\n",
    "    print(f\"{dictionary[word_id]} -> {weight:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.12.7",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
